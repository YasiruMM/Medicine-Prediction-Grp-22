{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMbrvg4GuXrlqi59q0HxGbP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YasiruMM/Medicine-Prediction-Grp-22/blob/Out-stock-predict/Flask_OOS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask flask-cors pyngrok joblib pandas numpy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umOUMwAH1A_W",
        "outputId": "06e34a1b-7e46-4c76-fbe7-2dceada0e4d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Collecting flask-cors\n",
            "  Downloading flask_cors-5.0.1-py3-none-any.whl.metadata (961 bytes)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.3-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.4.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->flask) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading flask_cors-5.0.1-py3-none-any.whl (11 kB)\n",
            "Downloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: pyngrok, flask-cors\n",
            "Successfully installed flask-cors-5.0.1 pyngrok-7.2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3I9WKxJpoLul"
      },
      "outputs": [],
      "source": [
        "from flask import Flask, request, jsonify\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from flask_cors import CORS  # To allow frontend access\n",
        "from pyngrok import ngrok\n",
        "import threading\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken 2tobmc7DDP8dlEKWTZyC0BOSf4m_6ha1VZVY5Xd7KJXWCp1QY\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DfvaslZ_oc9",
        "outputId": "8c9a7a15-10f1-4921-c687-6cb3e4b30d9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kY_kuJdJ2AeK",
        "outputId": "353ba84a-3ec3-4292-c808-a0ea0565a065"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "OOS_app = Flask(__name__)\n",
        "CORS(OOS_app)  # Enable CORS for frontend access\n",
        "\n",
        "# Load the trained Random Forest model\n",
        "rf_model = joblib.load(\"/content/drive/My Drive/DSGP/RandomForest_OOS.pkl\")\n",
        "\n",
        "# File paths for datasets\n",
        "features_file = \"/content/drive/My Drive/DSGP/NoiseHandled1_MediTrack_Dataset.csv\"\n",
        "predictions_file = \"/content/drive/My Drive/DSGP/XGBoost_Predictions.csv\"\n"
      ],
      "metadata": {
        "id": "XEWShvbz1W9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_merged_dataset():\n",
        "    try:\n",
        "        # Load datasets\n",
        "        df_features = pd.read_csv(features_file)\n",
        "        df_predictions = pd.read_csv(predictions_file)\n",
        "\n",
        "        # Transform XGBoost Predictions to Log Scale\n",
        "        prediction_cols = [\"Prediction 1\", \"Prediction 2\", \"Prediction 3\",\n",
        "                           \"Prediction 4\", \"Prediction 5\", \"Prediction 6\"]\n",
        "        df_predictions[prediction_cols] = np.log1p(df_predictions[prediction_cols])\n",
        "\n",
        "        # Keep Only Forecasted Drugs (Drugs that exist in Predictions)\n",
        "        df_filtered = df_features[df_features[\"Drug Name\"].isin(df_predictions[\"Drug Name\"])]\n",
        "\n",
        "\n",
        "        # Aggregate numeric features before merging using a dictionary.\n",
        "        agg_dict = {\n",
        "            \"Retail Price\": \"mean\",         # Average retail price\n",
        "            \"Purchase Price\": \"mean\",       # Average purchase price\n",
        "            \"Sales\": \"sum\",                 # Total sales\n",
        "            \"Mean Sales\": \"mean\",           # Mean sales\n",
        "            \"Buffer Stock\": \"mean\",         # Average buffer stock\n",
        "            \"Sales_LOESS_Date\": \"first\"  # Use first occurrence for Sales_LOESS_Date\n",
        "        }\n",
        "        df_features_agg = df_filtered.groupby(\"Drug Name\", as_index=False).agg(agg_dict)\n",
        "\n",
        "        # Merge aggregated features with predictions\n",
        "        df_merged = df_features_agg.merge(df_predictions, on=\"Drug Name\", how=\"left\")\n",
        "\n",
        "        # Drop duplicate Disease Category column and rename\n",
        "        df_merged.drop(columns=[\"Disease Category_y\"], inplace=True, errors=\"ignore\")\n",
        "        df_merged.rename(columns={\"Disease Category_x\": \"Disease Category\"}, inplace=True)\n",
        "\n",
        "        # Remove duplicates based on predictions\n",
        "        df_merged = df_merged.drop_duplicates(\n",
        "            subset=[\"Drug Name\", \"Prediction 1\", \"Prediction 2\",\n",
        "                    \"Prediction 3\", \"Prediction 4\", \"Prediction 5\", \"Prediction 6\"],\n",
        "            keep=\"first\"\n",
        "        )\n",
        "\n",
        "        # Compute log-transformed features:\n",
        "        df_merged[\"Log_Sales\"] = np.log1p(df_merged[\"Sales\"])\n",
        "        df_merged[\"Log_Retail_Price\"] = np.log1p(df_merged[\"Retail Price\"])\n",
        "        df_merged[\"Log_Purchase_Price\"] = np.log1p(df_merged[\"Purchase Price\"])\n",
        "        df_merged[\"Log_Buffer_Stock\"] = np.log1p(df_merged[\"Buffer Stock\"])\n",
        "\n",
        "        # Assign risk factors based on Disease Category\n",
        "        loss_factors = {\n",
        "            'Cardiovascular': 0.20,\n",
        "            'Diabetes': 0.25,\n",
        "            'Cholesterol': 0.15\n",
        "        }\n",
        "        df_merged['Loss Factor'] = df_merged['Disease Category'].map(loss_factors).fillna(0.15)\n",
        "        df_merged['Loss Quantity'] = df_merged['Loss Factor'] * df_merged[\"Log_Sales\"]\n",
        "        df_merged['Loss Quantity'] = df_merged['Loss Quantity'].apply(lambda x: max(0, x))\n",
        "\n",
        "        # Drop time-related features (if any)\n",
        "        df_merged.drop(columns=['Date', 'Month', 'Year'], inplace=True, errors=\"ignore\")\n",
        "\n",
        "        # Preserve Original Drug Name for API filtering\n",
        "        df_merged[\"Original Drug Name\"] = df_merged[\"Drug Name\"]\n",
        "\n",
        "        # One-Hot Encode categorical features: Disease Category and Drug Name\n",
        "        df_merged = pd.get_dummies(df_merged, columns=['Disease Category', 'Drug Name'], drop_first=True)\n",
        "\n",
        "        return df_merged\n",
        "\n",
        "    except Exception as e:\n",
        "        return None, str(e)\n"
      ],
      "metadata": {
        "id": "TJpExwGF2roJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@OOS_app.route('/get-drug-names', methods=['GET'])\n",
        "def get_drug_names():\n",
        "    try:\n",
        "        df_merged = generate_merged_dataset()\n",
        "        if df_merged is None:\n",
        "            return jsonify({\"error\": \"Failed to generate dataset!\"})\n",
        "        if \"Original Drug Name\" not in df_merged.columns:\n",
        "            return jsonify({\"error\": \"'Original Drug Name' column not found!\"})\n",
        "\n",
        "        available_drugs = df_merged[\"Original Drug Name\"].unique().tolist()\n",
        "        return jsonify({\"drug_names\": available_drugs})\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)})\n"
      ],
      "metadata": {
        "id": "-MxcFj9R3eDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@OOS_app.route('/predict-oos', methods=['POST'])\n",
        "def predict_oos():\n",
        "    try:\n",
        "        user_input = request.get_json()\n",
        "        user_drug = user_input.get(\"drug_name\")\n",
        "        user_loss = user_input.get(\"loss_quantity\")\n",
        "\n",
        "        df_merged = generate_merged_dataset()\n",
        "        if df_merged is None:\n",
        "            return jsonify({\"error\": \"Failed to generate dataset!\"})\n",
        "        if \"Original Drug Name\" not in df_merged.columns:\n",
        "            return jsonify({\"error\":\" 'Original Drug Name' column not found!\"})\n",
        "\n",
        "        if user_drug not in df_merged[\"Original Drug Name\"].values:\n",
        "            return jsonify({\"error\": f\"Drug '{user_drug}' not found!\"})\n",
        "        # injecting User inputs\n",
        "        X_user = df_merged[df_merged[\"Original Drug Name\"] == user_drug].copy()\n",
        "        X_user[\"Loss Quantity\"] = np.log1p(user_loss)  # Convert to log scale\n",
        "        features = ['Sales_LOESS_Date', 'Log_Sales', 'Log_Buffer_Stock', 'Log_Retail_Price',\n",
        "                    'Log_Purchase_Price', 'Loss Quantity'] + list(df_merged.columns[df_merged.columns.str.startswith(('Disease Category_', 'Drug Name_'))])\n",
        "\n",
        "        X_user = X_user[features]\n",
        "\n",
        "        y_pred_log = rf_model.predict(X_user)\n",
        "        y_pred_log=y_pred_log.reshape(-1)\n",
        "        y_pred_exp = np.expm1(y_pred_log)\n",
        "        # row wise adjusting the predictions\n",
        "        xgb_original = df_merged[df_merged[\"Original Drug Name\"] == user_drug][[\"Prediction 1\", \"Prediction 2\", \"Prediction 3\", \"Prediction 4\", \"Prediction 5\", \"Prediction 6\"]].values.flatten()\n",
        "        # Use the trained Random Forest model to predict the adjusted demand (risk quantity)\n",
        "        adjusted_predictions = y_pred_exp.flatten()  # Use ML-based predictions instead of simple subtraction\n",
        "\n",
        "\n",
        "       #Compare adjusted vs original to get risk\n",
        "        shortage_risk =  np.array(xgb_original)-np.array(adjusted_predictions)\n",
        "        shortage_risk_levels = [\"HIGH\" if val > 0 else \"LOW\" for val in shortage_risk]\n",
        "\n",
        "\n",
        "        result = {\n",
        "            \"drug_name\": user_drug,\n",
        "            \"loss_quantity\": user_loss,\n",
        "            \"original_predictions\": xgb_original.tolist(),\n",
        "            \"adjusted_predictions\": y_pred_exp.flatten().tolist(),\n",
        "            \"shortage_risk_levels\": shortage_risk_levels\n",
        "        }\n",
        "\n",
        "        return jsonify(result)\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)})\n"
      ],
      "metadata": {
        "id": "MUISCRcw4R6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start Flask in a background thread\n",
        "def run_flask():\n",
        "    OOS_app.run(port=5000)\n",
        "\n",
        "import threading\n",
        "thread = threading.Thread(target=run_flask)\n",
        "thread.start()\n",
        "\n",
        "# Get the new ngrok URL\n",
        "public_url = ngrok.connect(5000).public_url\n",
        "print(f\"Your Flask API is available at: {public_url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Sj82eTh9HUe",
        "outputId": "802f62d6-3984-47eb-c57e-844d2dfd8801"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your Flask API is available at: https://ca7d-35-227-120-109.ngrok-free.app\n"
          ]
        }
      ]
    }
  ]
}